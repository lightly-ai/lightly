{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This example requires the following dependencies to be installed:\n",
    "pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Note: The model and training settings do not follow the reference settings\n",
    "from the paper. The settings are chosen such that the example can easily be\n",
    "run on a small dataset with a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_small_patch16_224\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss, IBOTPatchLoss\n",
    "from lightly.models.modules import DINOProjectionHead, MaskedVisionTransformerTIMM\n",
    "from lightly.models.utils import (\n",
    "    random_block_mask,\n",
    "    update_drop_path_rate,\n",
    "    update_momentum,\n",
    ")\n",
    "from lightly.transforms.ibot_transform import IBOTTransform\n",
    "from lightly.utils.scheduler import cosine_schedule, linear_warmup_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_eval_module(module: Module) -> None:\n",
    "    \"\"\"Freeze the parameters of a module.\"\"\"\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "    module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBOT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.output_dim = 8192\n",
    "\n",
    "        # Backbones\n",
    "        vit_teacher = vit_small_patch16_224(\n",
    "            pos_embed=\"learn\",\n",
    "            dynamic_img_size=True,\n",
    "            init_values=1e-5,\n",
    "        )\n",
    "        self.teacher_backbone = MaskedVisionTransformerTIMM(\n",
    "            vit=vit_teacher,\n",
    "            antialias=False,\n",
    "            pos_embed_initialization=\"skip\",\n",
    "        )\n",
    "        self.student_backbone = copy.deepcopy(self.teacher_backbone)\n",
    "        update_drop_path_rate(\n",
    "            self.student_backbone.vit,\n",
    "            drop_path_rate=0.1,\n",
    "            mode=\"uniform\",\n",
    "        )\n",
    "        freeze_eval_module(self.teacher_backbone)\n",
    "        self.embed_dim = self.student_backbone.vit.embed_dim\n",
    "\n",
    "        # Projection heads\n",
    "        projection_head = partial(\n",
    "            DINOProjectionHead,\n",
    "            input_dim=self.embed_dim,\n",
    "            output_dim=self.output_dim,\n",
    "        )\n",
    "\n",
    "        self.student_head = projection_head(norm_last_layer=False)\n",
    "        self.student_cls_head = self.student_patch_head = self.student_head\n",
    "\n",
    "        self.teacher_head = projection_head()\n",
    "        self.teacher_cls_head = self.teacher_patch_head = self.teacher_head\n",
    "\n",
    "        freeze_eval_module(self.teacher_head)\n",
    "\n",
    "        # Losses\n",
    "        self.cls_criterion = DINOLoss(\n",
    "            output_dim=self.output_dim,\n",
    "            teacher_temp=0.07,\n",
    "        )\n",
    "        self.patch_criterion = IBOTPatchLoss(\n",
    "            output_dim=self.output_dim,\n",
    "            teacher_temp=0.07,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    def forward_teacher(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        features = self.teacher_backbone.encode(x)\n",
    "        cls_tokens = features[:, 0]\n",
    "        return cls_tokens, features\n",
    "\n",
    "    def forward_student(\n",
    "        self, x: Tensor, mask: Tensor | None\n",
    "    ) -> tuple[Tensor, Tensor | None]:\n",
    "        features = self.student_backbone.encode(x, mask=mask)\n",
    "        cls_tokens = features[:, 0]\n",
    "        masked_features = None if mask is None else features[mask]\n",
    "        return cls_tokens, masked_features\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[list[Tensor], Tensor, list[str]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        views, _ = batch[0], batch[1]\n",
    "        global_views = torch.cat(views[:2])\n",
    "        local_views = torch.cat(views[2:])\n",
    "\n",
    "        # Masking\n",
    "        B = len(global_views)\n",
    "        sequence_length = self.teacher_backbone.sequence_length\n",
    "        mask = global_views.new_zeros((B, sequence_length), dtype=torch.bool)\n",
    "        # Mask patches except class token.\n",
    "        H, W = self.teacher_backbone.vit.patch_embed.grid_size\n",
    "        assert (\n",
    "            H * W == sequence_length - 1\n",
    "        ), f\"Unexpected grid size: {H}x{W}, sequence_length {sequence_length}\"\n",
    "        block_mask = random_block_mask(size=(B, H, W), device=mask.device)\n",
    "        mask[:, 1:] = block_mask.flatten(start_dim=1)\n",
    "\n",
    "        # Teacher forward\n",
    "        with torch.no_grad():\n",
    "            teacher_cls_token, teacher_features = self.forward_teacher(global_views)\n",
    "            teacher_cls_out = self.teacher_cls_head.forward(teacher_cls_token)\n",
    "            teacher_masked_out = self.teacher_patch_head.forward(teacher_features[mask])\n",
    "\n",
    "        # Student forward\n",
    "        student_global_cls_token, student_global_masked_features = self.forward_student(\n",
    "            global_views, mask=mask\n",
    "        )\n",
    "        student_global_cls_out = self.student_cls_head.forward(student_global_cls_token)\n",
    "        student_global_masked_out = self.student_patch_head.forward(\n",
    "            student_global_masked_features\n",
    "        )\n",
    "\n",
    "        student_local_cls_token, _ = self.forward_student(local_views, mask=None)\n",
    "        student_local_cls_out = self.student_head.forward(student_local_cls_token)\n",
    "        student_cls_out = torch.cat([student_global_cls_out, student_local_cls_out])\n",
    "\n",
    "        teacher_temp = linear_warmup_schedule(\n",
    "            step=self.trainer.global_step,\n",
    "            warmup_steps=int(\n",
    "                30 / self.trainer.max_epochs * self.trainer.estimated_stepping_batches\n",
    "            ),\n",
    "            start_value=0.04,\n",
    "            end_value=0.07,\n",
    "        )\n",
    "        cls_loss = self.cls_criterion(\n",
    "            teacher_out=teacher_cls_out.chunk(2),\n",
    "            student_out=student_cls_out.chunk(len(views)),\n",
    "            teacher_temp=teacher_temp,\n",
    "        )\n",
    "        patch_loss = self.patch_criterion(\n",
    "            teacher_out=teacher_masked_out,\n",
    "            student_out=student_global_masked_out,\n",
    "            mask=block_mask,\n",
    "            teacher_temp=teacher_temp,\n",
    "        )\n",
    "        loss = cls_loss + patch_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = AdamW(self.parameters(), lr=0.001)\n",
    "        return optim\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer: AdamW, *args) -> None:\n",
    "        # Cancel gradients of the last layer of the student head\n",
    "        self.student_head.cancel_last_layer_gradients(self.current_epoch)\n",
    "\n",
    "        # Apply weight decay schedule\n",
    "        weight_decay = cosine_schedule(\n",
    "            step=self.trainer.global_step,\n",
    "            max_steps=self.trainer.estimated_stepping_batches,\n",
    "            start_value=0.04,\n",
    "            end_value=0.4,\n",
    "        )\n",
    "        for group in optimizer.param_groups:\n",
    "            if group[\"weight_decay\"] != 0.0:\n",
    "                group[\"weight_decay\"] = weight_decay\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        # Momentum update teacher.\n",
    "        momentum = cosine_schedule(\n",
    "            step=self.trainer.global_step,\n",
    "            max_steps=self.trainer.estimated_stepping_batches,\n",
    "            start_value=0.996,\n",
    "            end_value=1.0,\n",
    "        )\n",
    "        update_momentum(self.student_backbone, self.teacher_backbone, m=momentum)\n",
    "        update_momentum(self.student_head, self.teacher_head, m=momentum)\n",
    "\n",
    "        return super().on_train_batch_end(outputs, batch, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IBOT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = IBOTTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "def target_transform(t):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.VOCDetection(\n",
    "    \"datasets/pascal_voc\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=50, devices=1, accelerator=accelerator)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
